[
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html#title-2-header",
    "href": "index.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Projects/project3.html",
    "href": "Projects/project3.html",
    "title": "Client Report - Project 3",
    "section": "",
    "text": "Our investigation into the performance metrics of MLB players who attended BYU-Idaho and the comparative analysis of two prominent baseball teams, the Boston Red Sox and the Baltimore Orioles, has unearthed valuable insights. These findings offer strategic implications for player development, team management, and the financial dynamics of team performance.\n\n\nRead and format project data\n# Include and execute your code here\nsqlite_file = 'lahmansbaseballdb.sqlite'\nconn = sqlite3.connect(sqlite_file)",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#elevator-pitch",
    "href": "Projects/project3.html#elevator-pitch",
    "title": "Client Report - Project 3",
    "section": "",
    "text": "Our investigation into the performance metrics of MLB players who attended BYU-Idaho and the comparative analysis of two prominent baseball teams, the Boston Red Sox and the Baltimore Orioles, has unearthed valuable insights. These findings offer strategic implications for player development, team management, and the financial dynamics of team performance.\n\n\nRead and format project data\n# Include and execute your code here\nsqlite_file = 'lahmansbaseballdb.sqlite'\nconn = sqlite3.connect(sqlite_file)",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#questiontask-1",
    "href": "Projects/project3.html#questiontask-1",
    "title": "Client Report - Project 3",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nWrite an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\nOur analysis focused on creating a dataframe detailing baseball players who attended BYU-Idaho. The table includes columns such as playerID, schoolID, salary, and the yearID/teamID associated with each salary. This data revealed that the highest salary paid was approximately $4,000,000 in 2014, while the lowest was around $150,000 in 1997. Notably, nearly half of the players who attended BYU-Idaho earned over $1 million annually.\n\n\nRead and format data\n# Include and execute your code here\nq = '''\nSELECT DISTINCT cp.playerID, s.salary, s.yearID, s.teamID, cp.schoolID -- this selects the columns\nFROM collegeplaying AS cp \nLEFT JOIN salaries AS s \nON s.playerID = cp.PlayerID\nWHERE cp.schoolID = 'idbyuid'\nORDER BY s.salary DESC;\n'''\n\ndf1 = pd.read_sql_query(q,conn)\ndf1\n\n\n\n\n\n\n\n\n\nplayerID\nsalary\nyearID\nteamID\nschoolID\n\n\n\n\n0\nlindsma01\n4000000.0\n2014.0\nCHA\nidbyuid\n\n\n1\nlindsma01\n3600000.0\n2012.0\nBAL\nidbyuid\n\n\n2\nlindsma01\n2800000.0\n2011.0\nCOL\nidbyuid\n\n\n3\nlindsma01\n2300000.0\n2013.0\nCHA\nidbyuid\n\n\n4\nlindsma01\n1625000.0\n2010.0\nHOU\nidbyuid\n\n\n5\nstephga01\n1025000.0\n2001.0\nSLN\nidbyuid\n\n\n6\nstephga01\n900000.0\n2002.0\nSLN\nidbyuid\n\n\n7\nstephga01\n800000.0\n2003.0\nSLN\nidbyuid\n\n\n8\nstephga01\n550000.0\n2000.0\nSLN\nidbyuid\n\n\n9\nlindsma01\n410000.0\n2009.0\nFLO\nidbyuid\n\n\n10\nlindsma01\n395000.0\n2008.0\nFLO\nidbyuid\n\n\n11\nlindsma01\n380000.0\n2007.0\nFLO\nidbyuid\n\n\n12\nstephga01\n215000.0\n1999.0\nSLN\nidbyuid\n\n\n13\nstephga01\n185000.0\n1998.0\nPHI\nidbyuid\n\n\n14\nstephga01\n150000.0\n1997.0\nPHI\nidbyuid\n\n\n15\ncatetr01\nNaN\nNaN\nNone\nidbyuid",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#questiontask-2",
    "href": "Projects/project3.html#questiontask-2",
    "title": "Client Report - Project 3",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nThis three-part question requires you to calculate batting average (number of hits divided by the number of at-bats)\nOur first query calculated batting averages (hits divided by at-bats) for players with at least 1 at-bat in a given year. A noteworthy observation is that a career average of about .360 is considered excellent. A higher average might indicate a shorter career or a player who didn’t accumulate many hits, especially for pitchers.\n\n\nRead and format data\n# Include and execute your code here\nq = '''\nSELECT playerID, yearID, CAST(SUM(H) AS FLOAT) / CAST(SUM(AB) AS FLOAT) AS batting_average\nFROM Batting\nGROUP BY playerID, yearID\nHAVING SUM(AB) &gt;= 1\nORDER BY batting_average DESC, playerID ASC\nLIMIT 5;\n\n'''\ndf2 = pd.read_sql_query(q,conn)\ndf2\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nbatting_average\n\n\n\n\n0\nabernte02\n1960\n1.0\n\n\n1\nabramge01\n1923\n1.0\n\n\n2\nacklefr01\n1964\n1.0\n\n\n3\nalanirj01\n2019\n1.0\n\n\n4\nalberan01\n2017\n1.0\n\n\n\n\n\n\n\nIn the second part of the query, we refined the criteria to include only players with at least 10 at-bats in a given year, providing a more stringent measure for batting average.\n::: {#cell-Q2 chart .cell execution_count=5}\n\nplot example\n# Include and execute your code here\nq = '''\nSELECT playerID, yearID, CAST(SUM(H) AS FLOAT) / CAST(SUM(AB) AS FLOAT) AS batting_average\nFROM Batting\nGROUP BY playerID, yearID\nHAVING SUM(AB) &gt;= 10\nORDER BY batting_average DESC, playerID ASC\nLIMIT 5;\n'''\ndf2 = pd.read_sql_query(q,conn)\ndf2\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nbatting_average\n\n\n\n\n0\nnymanny01\n1974\n0.642857\n\n\n1\ncarsoma01\n2013\n0.636364\n\n\n2\naltizda01\n1910\n0.600000\n\n\n3\nsilvech01\n1948\n0.571429\n\n\n4\npuccige01\n1930\n0.562500\n\n\n\n\n\n\n:::\nFinally, we calculated the batting average for players over their entire careers (all years combined), including only players with at least 100 at-bats. The top 5 results provide insights into consistent performance across multiple seasons.\n::: {#cell-Q2 table .cell .tbl-cap-location-top execution_count=6}\n\ntable example\n# Include and execute your code here\nq = '''\nSELECT playerID, CAST(SUM(H) AS FLOAT) / CAST(SUM(AB) AS FLOAT) AS career_batting_average\nFROM Batting\nGROUP BY playerID\nHAVING SUM(AB) &gt;= 100\nORDER BY career_batting_average DESC\nLIMIT 5;\n\n'''\n\ndf2 = pd.read_sql_query(q,conn)\ndf2\n\n\n\n\n\n\n\n\n\nplayerID\ncareer_batting_average\n\n\n\n\n0\ncobbty01\n0.366299\n\n\n1\nbarnero01\n0.359682\n\n\n2\nhornsro01\n0.358497\n\n\n3\njacksjo01\n0.355752\n\n\n4\nmeyerle01\n0.355509\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#questiontask-3",
    "href": "Projects/project3.html#questiontask-3",
    "title": "Client Report - Project 3",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nPick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc). Write an SQL query to get the data you need, then make a graph using Plotly Express to visualize the comparison. What do you learn?\nOur comparative analysis focused on the Boston Red Sox and the Baltimore Orioles, employing average salary and number of wins as key metrics. Notably, Boston’s higher average salary correlated with improved performance, as reflected in the win-loss records. The visual representation using Plotly Express underscores the impact of team investment on overall success.\n\n\nRead and format data\n# Include and execute your code here\nq = '''\nSELECT yearID AS Year, teamID AS Team, AVG(salary) AS AVG_Salary\nFROM salaries\nWHERE teamID = 'BAL' OR teamID = 'BOS'\nGROUP BY yearID, teamID\nORDER BY yearID\n'''\n\ndf3 = pd.read_sql_query(q,conn)\ndf3\n\n\n\n\n\n\n\n\n\nYear\nTeam\nAVG_Salary\n\n\n\n\n0\n1985\nBAL\n5.254869e+05\n\n\n1\n1985\nBOS\n4.359024e+05\n\n\n2\n1986\nBAL\n4.483192e+05\n\n\n3\n1986\nBOS\n4.966289e+05\n\n\n4\n1987\nBAL\n4.633424e+05\n\n\n...\n...\n...\n...\n\n\n59\n2014\nBOS\n4.484514e+06\n\n\n60\n2015\nBAL\n4.108744e+06\n\n\n61\n2015\nBOS\n5.659481e+06\n\n\n62\n2016\nBAL\n5.581498e+06\n\n\n63\n2016\nBOS\n6.501578e+06\n\n\n\n\n64 rows × 3 columns\n\n\n\n\n\nShow the code\nq = '''\nSELECT yearID AS Year, teamID AS Team, SUM(W) AS Wins\nFROM pitching\nWHERE teamID = 'BAL' OR teamID ='BOS'\nGROUP BY yearID, teamID\nHAVING yearID &gt;= 1985 AND yearID &lt;= 2016\nORDER BY yearID\n'''\n\ndf4 = pd.read_sql_query(q,conn)\ndf4\n\n\n\n\n\n\n\n\n\nYear\nTeam\nWins\n\n\n\n\n0\n1985\nBAL\n83\n\n\n1\n1985\nBOS\n81\n\n\n2\n1986\nBAL\n73\n\n\n3\n1986\nBOS\n95\n\n\n4\n1987\nBAL\n67\n\n\n...\n...\n...\n...\n\n\n59\n2014\nBOS\n71\n\n\n60\n2015\nBAL\n81\n\n\n61\n2015\nBOS\n78\n\n\n62\n2016\nBAL\n89\n\n\n63\n2016\nBOS\n93\n\n\n\n\n64 rows × 3 columns\n\n\n\n::: {#cell-Q3 chart .cell execution_count=9}\n\nplot example\n# Include and execute your code here\nchart = px.line(df3.head(70),\n    x=\"Year\", \n    y=\"AVG_Salary\",\n    color='Team',\n    title='Average Salary throughout the Years'\n)\n\nchart.add_annotation(x=2011, y=5991203, text=\"Salary Spike in 2011\", font=dict(size=10), arrowcolor=\"black\")\n\nchart.show()\n\n\n                                                \n\n:::\n::: {#cell-Q3 chart2 .cell execution_count=10}\n\nplot example\n# Include and execute your code here\nchart = px.line(df4.head(200),\n    x=\"Year\", \n    y=\"Wins\",\n    color='Team',\n    title='Wins Each Year'\n)\n\nchart.add_annotation(x=2011, y=90, text=\"2011\", font=dict(size=10), arrowcolor=\"black\")\n\nchart.show()\n\n\n                                                \n\n:::\nConclusion\nIn conclusion, our analysis provides actionable insights for both individual player development and team management strategies. The correlation between investment in player salaries and team performance highlights the intricate dynamics of success in professional baseball. Further exploration and refinement of these findings could offer a competitive edge in the highly dynamic and challenging landscape of Major League Baseball.",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project1.html",
    "href": "Projects/project1.html",
    "title": "Client Report - Project 1",
    "section": "",
    "text": "A name can give you a lot of information on a person. With the right data set you can predict the most likely time and location of ones birth. This kind of data can also show how world events can effect the popularity of names, such as major world figures or generational trends.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#elevator-pitch",
    "href": "Projects/project1.html#elevator-pitch",
    "title": "Client Report - Project 1",
    "section": "",
    "text": "A name can give you a lot of information on a person. With the right data set you can predict the most likely time and location of ones birth. This kind of data can also show how world events can effect the popularity of names, such as major world figures or generational trends.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-1",
    "href": "Projects/project1.html#questiontask-1",
    "title": "Client Report - Project 1",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nHow does your name at your birth year compare to its use historically\nThe following graph shows that the use of the name Carter has been on a continues rise in popularity since the early 1900s. Prior to this the name was relativly unused. The name peaked in popularity in 2011. So when I was named in 2001 the name was on a major rise in popularity.\n\n\nRead and format data\n# Include and execute your code here\ncarter_data = df[df['name'] == 'Carter']\n\n# Sum occurrences across all states for each year\ntotal_occurrences = carter_data.groupby('year').sum(numeric_only=True).reset_index()\n\n\n__\n::: {#cell-Q1 chart .cell execution_count=4}\n\nplot example\n# Include and execute your code here\n# Filter data for the name \"Carter\"\n\n\n# Plot the line chart using Plotly Express\nfig = px.line(total_occurrences, x='year', y='Total',\n              labels={'Total': 'Total Occurrences'}, title='Occurrences of Name \"Carter\" Over Years')\n\nhighlighted_point = total_occurrences[total_occurrences['year'] == 2001]\nfig.add_trace(px.scatter(highlighted_point, x='year', y='Total').data[0])\n\nfig.update_traces(marker=dict(size=10, color='red'))\n\nfig.show()\n\n\n                                                \n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-2",
    "href": "Projects/project1.html#questiontask-2",
    "title": "Client Report - Project 1",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nIf you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\nBased on the data, if you were talk to someone named Brittany it is most likely that they would be between the ages of 20 to 40. The name Brittany began a large rise in popularity in the 80s before peaking in 1990 and regressing at around the same rate. \n\n\nRead and format data\n# Include and execute your code here\nbrittany_data = df[df['name'] == 'Brittany']\n\n# Sum occurrences across all states for each year\ntotal_occurrences_brittany = brittany_data.groupby('year').sum(numeric_only=True).reset_index()\n\n\n::: {#cell-Q2 chart .cell execution_count=6}\n\nplot example\n# Include and execute your code here\n# Filter data for the name \"Brittany\"\n\n\n# Plot the line chart using Plotly Express\nfig = px.line(total_occurrences_brittany, x='year', y='Total',\n              labels={'Total': 'Total Occurrences'}, title='Occurrences of Name \"Brittany\" Over Years')\nfig.show()\n\n\n                                                \n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-3",
    "href": "Projects/project1.html#questiontask-3",
    "title": "Client Report - Project 1",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nMary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names. What trends do you notice?\nInerestingly, there has been a large decline in popularity of each of these primarily Christan names. It is widly know that christianity has been on a decline in recent years. That fact could be relate to this data although more informatoin would be needed to confirm that theory.\n\n\nRead and format data\n# Include and execute your code here\nnames_to_include = ['Mary', 'Martha', 'Peter', 'Paul']\ndf_filtered = df[df['name'].isin(names_to_include)]\n\n\n::: {#cell-Q3 chart .cell execution_count=8}\n\nplot example\n#| fig-align: center\n# Include and execute your code here\n\n\n\n# Plot the line chart using Plotly Express\nfig = px.line(df_filtered, x='year', y='Total', color='name',\n              labels={'occurrences': 'Occurrences'},\n              title='Name Usage Comparison (1920 - 2000)')\nfig.show()\n\n\n                                                \n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-4",
    "href": "Projects/project1.html#questiontask-4",
    "title": "Client Report - Project 1",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nThink of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\nIn looking at thedata for the name Marty, referenceing Back to the Future, its hard to say if the movie had an effect on the name. The name prior to the year of the movies release was on a consistent decline in usage. However, after the movies release in 1985 the usage flatened out and has stayed relativley consistent from year to year with around 100 - 130 usaes each year.\n\n\nRead and format data\n# Include and execute your code here\n# Plot the line chart using Plotly Express\nselected_name = 'Marty'\nstart_year = 1980\nend_year = 1990\n\nfiltered_data = df[(df['name'] == selected_name) & (df['year'].between(start_year, end_year))]\n\n\n::: {#cell-Q4 chart .cell execution_count=10}\n\nplot example\n#| fig-align: center\n# Include and execute your code here\n\n# Create a line chart\nfig = px.line(filtered_data, x='year', y='Total',\n              labels={'Total': f'Total Occurrences of {selected_name}'},\n              title=f'Occurrences of Name \"{selected_name}\" Over Years')\n\n# Add a vertical line at the year of the movie release\nfig.add_shape(\n    dict(\n        type=\"line\",\n        x0=1985,\n        x1=1985,\n        y0=0,\n        y1=filtered_data['Total'].max(),\n        line=dict(color=\"red\", dash=\"dash\"),\n    )\n)\n\nfig.show()\n\n\n                                                \n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project4.html",
    "href": "Projects/project4.html",
    "title": "Client Report - Project 4",
    "section": "",
    "text": "This report highlights a correlation between construction year and property attributes, revealing a surge in livable space density and net prices after 1980. Analysis showcases significant relationships between house size, construction year, and net price. Moreover, a classification model, notably Report 2, demonstrates superior predictive capability, with high precision, recall, and F1-scores. Crucial features such as architectural style and garage type play pivotal roles in predicting property outcomes. Overall, the report provides valuable insights for stakeholders in the real estate industry.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\")",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#elevator-pitch",
    "href": "Projects/project4.html#elevator-pitch",
    "title": "Client Report - Project 4",
    "section": "",
    "text": "This report highlights a correlation between construction year and property attributes, revealing a surge in livable space density and net prices after 1980. Analysis showcases significant relationships between house size, construction year, and net price. Moreover, a classification model, notably Report 2, demonstrates superior predictive capability, with high precision, recall, and F1-scores. Crucial features such as architectural style and garage type play pivotal roles in predicting property outcomes. Overall, the report provides valuable insights for stakeholders in the real estate industry.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\")",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-1",
    "href": "Projects/project4.html#questiontask-1",
    "title": "Client Report - Project 4",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCreate 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.\n_After 1980, there’s a noticeable increase in both the livable space density and the net price of homes, implying a potential correlation between the construction year and the size and cost of properties. The first scatter plot illustrates a correlation between livable area and the year of construction. There’s a significant increase in the number of houses larger than 5000 square feet built after 1980, indicating a correlation between house size and the year built.\nThe second chart compares the net price and the construction year, revealing that higher net prices are more common in homes built after 1980. This suggests a relationship between construction year and net price._\n::: {#cell-Q1 chart 1 .cell execution_count=3}\n\nPlot liveable area versus house build year\n# Include and execute your code here\nsubset = df[['before1980', 'yrbuilt', 'livearea']]\nsubset['color'] = subset['before1980'].apply(lambda x: 'Before 1980' if x else 'After 1980')\ncolor_map_labels = {'Before 1980': 'blue', 'After 1980': 'red'}\nchart = px.scatter(subset, x=\"yrbuilt\", y=\"livearea\", color=\"color\", color_discrete_map=color_map_labels,\n                   title='Liveable Area versus Year Built')\nchart.update_layout(xaxis_title=\"Year Built\", yaxis_title=\"Liveable Area\")\nchart.show()\n\n\n                                                \n\n:::\nThe chart indicates a notable surge in the number of homes with larger livable areas after 1980.\n::: {#cell-Q1 chart 2 .cell execution_count=4}\n\nPlot net price versus build year\n# Include and execute your code here\nsubset = df[['before1980', 'yrbuilt', 'netprice']]\nsubset['color'] = subset['before1980'].apply(lambda x: 'Before 1980' if x else 'After 1980')\ncolor_map_labels = {'Before 1980': 'blue', 'After 1980': 'red'}\nchart = px.scatter(subset, x=\"yrbuilt\", y=\"netprice\", color=\"color\", color_discrete_map=color_map_labels,\n                   title=\"Net Price versus Year Built\")\nchart.update_layout(xaxis_title=\"Year Built\", yaxis_title=\"Net Price\")\nchart.show()\n\n\n                                                \n\n:::\nThis chart also illustrates a rise in the number of higher-priced homes after 1980. While there are a few outliers in the data before 1980, they are insufficient to significantly alter the overall trend.",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-2",
    "href": "Projects/project4.html#questiontask-2",
    "title": "Client Report - Project 4",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nBuild a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.\n_Report 2 demonstrates the highest performance across all evaluation metrics, boasting the highest accuracy, precision, recall, and F1-score among the three models evaluated. With precision and recall scores of 0.90 and 0.94 for Class 0 and Class 1 respectively, Report 2 effectively identifies both positive and negative instances.\nThe balanced F1-score of 0.90 and 0.94 for Class 0 and Class 1 confirms the model’s ability to maintain a trade-off between precision and recall. Overall, Report 2 emerges as the most robust classifier, showcasing superior predictive capability on the dataset._\n\n\nSplit data and train models\n# Include and execute your code here\nX_pred = df.drop(df.filter(regex='before1980|yrbuilt|parcel').columns, axis=1)\ny_pred = df.filter(regex=\"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(X_pred, y_pred, test_size=.32, random_state=78)\n\n\n\n\nShow the code\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n\n\n\n\nShow the code\nprint(metrics.classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       0.86      0.89      0.88      2713\n           1       0.94      0.91      0.92      4620\n\n    accuracy                           0.91      7333\n   macro avg       0.90      0.90      0.90      7333\nweighted avg       0.91      0.91      0.91      7333\n\n\n\n\n\nShow the code\nclf = RandomForestClassifier()\nclf = clf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n\n\n\n\nShow the code\nprint(metrics.classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       0.90      0.90      0.90      2713\n           1       0.94      0.94      0.94      4620\n\n    accuracy                           0.93      7333\n   macro avg       0.92      0.92      0.92      7333\nweighted avg       0.93      0.93      0.93      7333\n\n\n\n\n\nShow the code\nclf = GradientBoostingClassifier()\nclf = clf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n\n\n\n\nShow the code\nprint(metrics.classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       0.88      0.86      0.87      2713\n           1       0.92      0.93      0.92      4620\n\n    accuracy                           0.90      7333\n   macro avg       0.90      0.89      0.89      7333\nweighted avg       0.90      0.90      0.90      7333",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-3",
    "href": "Projects/project4.html#questiontask-3",
    "title": "Client Report - Project 4",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nJustify your classification model by discussing the most important features selected by your model. This discussion should include a chart and a description of the features.\n_In the classification model, several features are crucial for predicting outcomes. The most significant feature is the architectural style, labeled as “one-story,” with an importance value of 0.34. This feature indicates whether a property follows a one-story architectural design, suggesting its pivotal role in the model’s decision-making process.\nAdditionally, the presence of an attached garage (gartype_Att) emerges as the second most influential feature, highlighting its importance in determining property classifications. Quality rating (quality_C) and living area size (livearea) also show notable importance values, further enhancing the model’s predictive accuracy.\nThese features’ prevalence underscores their significance in characterizing properties and their potential impact on predicted outcomes, providing valuable insights into the factors driving the classification decisions._\n\n\nShow the code\n# feature_importance = clf.feature_importances_\n# feature_names = X_train.columns\n\n# # Create a dictionary to associate feature names with their importance values\n# feature_importance_dict = dict(zip(feature_names, feature_importance))\n\n# # Sort the dictionary by importance values in descending order\n# sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n\n# # Display the top features and their importance values\n# for feature, importance in sorted_feature_importance:\n#     print(f\"Feature: {feature}, Importance: {importance}\")\n\n\n\n\nExtract feature importance\n# Include and execute your code here\ndf_features = pd.DataFrame({'f_names': X_train.columns, 'f_values': clf.feature_importances_}).sort_values('f_values', ascending=False)\n\n\n::: {#cell-Q3 feature importance chart .cell execution_count=14}\n\nPlot feature importance\n# Include and execute your code here\nchart = px.bar(df_features.head(10), x='f_values', y='f_names')\nchart.update_layout(yaxis={'categoryorder': 'total ascending'})\nchart.show()\n\n\n                                                \n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-4",
    "href": "Projects/project4.html#questiontask-4",
    "title": "Client Report - Project 4",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nDescribe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.\n_The classification model demonstrates robust performance, evident through its high precision, recall, and F1-score values across both classes. Precision measures the proportion of true positive predictions out of all positive predictions made by the model, indicating its ability to accurately identify positive instances while minimizing false positives.\nRecall, on the other hand, measures the model’s effectiveness in capturing actual positive instances, with higher values suggesting fewer false negatives. The F1-score, being the mean of precision and recall, provides a balanced assessment of the model’s overall accuracy, considering both precision and recall simultaneously.\nWith precision rates of 0.90 for class 0 and 0.94 for class 1, and recall rates of 0.90 for class 0 and 0.94 for class 1, coupled with balanced F1-scores of 0.90 for class 0 and 0.94 for class 1, the model demonstrates its reliability in making precise predictions for each class while maintaining a strong balance between precision and recall._",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Carter Sperry’s CV",
    "section": "",
    "text": "Student at BYU-I.\n\n&lt;a href=“https://www.linkedin.com/in/carter-sperry-947430148/\n\n\n\nStuding Computer Science and IT at Brigham Young University - Idaho\n\n\n\n2015 - 2019 Mountain View High School.\n2021 - now BYU-I\n\n\n\n2015-2018 Meridian Pools, Meridian ID\n\nLifegaurd\nSwim Instructer\n\n2023 Amazon, Boise ID\n\nWarehouse Associate"
  },
  {
    "objectID": "resume.html#currently",
    "href": "resume.html#currently",
    "title": "Carter Sperry’s CV",
    "section": "",
    "text": "Studing Computer Science and IT at Brigham Young University - Idaho"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Carter Sperry’s CV",
    "section": "",
    "text": "2015 - 2019 Mountain View High School.\n2021 - now BYU-I"
  },
  {
    "objectID": "resume.html#job-history",
    "href": "resume.html#job-history",
    "title": "Carter Sperry’s CV",
    "section": "",
    "text": "2015-2018 Meridian Pools, Meridian ID\n\nLifegaurd\nSwim Instructer\n\n2023 Amazon, Boise ID\n\nWarehouse Associate"
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Projects/project5.html",
    "href": "Projects/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#elevator-pitch",
    "href": "Projects/project5.html#elevator-pitch",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask-1",
    "href": "Projects/project5.html#questiontask-1",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCOPY PASTE QUESTION|TASK 1 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q1 chart .cell execution_count=4}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q1 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=5}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask-2",
    "href": "Projects/project5.html#questiontask-2",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nCOPY PASTE QUESTION|TASK 2 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q2 chart .cell execution_count=7}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q2 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=8}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask-3",
    "href": "Projects/project5.html#questiontask-3",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nCOPY PASTE QUESTION|TASK 3 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q3 chart .cell execution_count=10}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q3 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=11}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project2.html",
    "href": "Projects/project2.html",
    "title": "Client Report - Late flights and Missing Data",
    "section": "",
    "text": "In this analysis, I delve into a comprehensive examination of flight delays. Leveraging SQL techniques, I address data inconsistencies, perform insightful analyses, and generate meaningful visualizations to derive key insights.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_json(\"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\")\n\n\n\n\nShow the code\ndef missing_checks(df, column ):\n    out1 = df[column].isnull().sum(axis = 0)\n    out2 = df[column].describe()\n    out3 = df[column].describe(exclude=np.number)\n    print('\\n\\n\\n')\n    print('Checking column' + column)\n    print('\\n')\n    print('Missing summary')\n    print(out1)\n    print('\\n')\n    print(\"Numeric summaries\")\n    print(out2)\n    print('\\n')\n    print('Non Numeric summaries')\n    print(out3)",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#elevator-pitch",
    "href": "Projects/project2.html#elevator-pitch",
    "title": "Client Report - Late flights and Missing Data",
    "section": "",
    "text": "In this analysis, I delve into a comprehensive examination of flight delays. Leveraging SQL techniques, I address data inconsistencies, perform insightful analyses, and generate meaningful visualizations to derive key insights.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_json(\"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\")\n\n\n\n\nShow the code\ndef missing_checks(df, column ):\n    out1 = df[column].isnull().sum(axis = 0)\n    out2 = df[column].describe()\n    out3 = df[column].describe(exclude=np.number)\n    print('\\n\\n\\n')\n    print('Checking column' + column)\n    print('\\n')\n    print('Missing summary')\n    print(out1)\n    print('\\n')\n    print(\"Numeric summaries\")\n    print(out2)\n    print('\\n')\n    print('Non Numeric summaries')\n    print(out3)",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-1",
    "href": "Projects/project2.html#questiontask-1",
    "title": "Client Report - Late flights and Missing Data",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”). In your report include one record example (one row) from your new data, in the raw JSON format. Your example should display the “NaN” for at least one missing value.\n\n\nShow the code\ndf_clean = df\n\n\n\n\nShow the code\ndf_clean.month.replace(['NaN', 'n/a'], np.nan, inplace = True)\ndf_clean.month.replace('Febuary', 'February', inplace = True)\nmean = round(df_clean.minutes_delayed_carrier.mean(), 2)\ndf_clean.minutes_delayed_carrier.replace(np.nan, mean, inplace = True)\ndf_clean.num_of_delays_late_aircraft.replace(-999, 0, inplace = True)\ndf_clean.num_of_delays_carrier.replace(\"1500+\", 1500, inplace= True)\ndf_clean.airport_name.replace('', 'NaN', inplace = True)\n\ndf_clean.month = df_clean.month.replace('n/a', np.nan)\ndf_clean[\"month\"] = df_clean[\"month\"].ffill()\n\ndf_clean.iloc[2]\n\n\nairport_code                          IAD\nairport_name                          NaN\nmonth                             January\nyear                               2005.0\nnum_of_flights_total                12381\nnum_of_delays_carrier                 414\nnum_of_delays_late_aircraft          1058\nnum_of_delays_nas                     895\nnum_of_delays_security                  4\nnum_of_delays_weather                  61\nnum_of_delays_total                  2430\nminutes_delayed_carrier          51902.25\nminutes_delayed_late_aircraft       70919\nminutes_delayed_nas               35660.0\nminutes_delayed_security              208\nminutes_delayed_weather              4497\nminutes_delayed_total              134881\nName: 2, dtype: object",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-2",
    "href": "Projects/project2.html#questiontask-2",
    "title": "Client Report - Late flights and Missing Data",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nWhich airport has the worst delays? Discuss the metric you chose, and why you chose it to determine the “worst” airport. Your answer should include a summary table that lists (for each airport) the total number of flights, total number of delayed flights, proportion of delayed flights, and average delay time in hours.\nLooking at the average delay time for the various airports in the database, it can be seen that the Chicago O’Hare International (ORD) and the San Francisco International (SFO) airports generate the longest delayed flights on average.\n\n\nRead and format data\n# Include and execute your code here\n\ndf_clean['prop_delayed_flights'] = df_clean['num_of_delays_total'] / df_clean['num_of_flights_total']\n\ndf_clean['avg_delay_hour'] = (df_clean['minutes_delayed_total'] / df_clean['num_of_delays_total']) / 60\n\nworst = df_clean.filter(['airport_code', 'airport_name', 'month', 'year', 'num_of_flights_total', 'num_of_delays_total', 'prop_delayed_flights', 'avg_delay_hour'])\n\nworst.head(10)\n\npx.bar(worst, x = 'airport_code', y =  ['num_of_flights_total', 'num_of_delays_total'], barmode = 'group')\n\nhour = worst.groupby('airport_code')['avg_delay_hour'].mean()\nfig1 = px.bar(hour)\nfig1.update_xaxes(title = \"Airport\")\nfig1.update_yaxes(title = \"Average Hours Delayed\")\nfig1.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-3",
    "href": "Projects/project2.html#questiontask-3",
    "title": "Client Report - Late flights and Missing Data",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nWhat is the best month to fly if you want to avoid delays of any length? Discuss the metric you chose and why you chose it to calculate your answer. Include one chart to help support your answer, with the x-axis ordered by month.\nIn order to have the best chance to avoid any flight delays, the best months to travel are September and November. Both months show signifgantly reduced total delays.\n\n\nRead and format data\n# Include and execute your code here\n\n\n# Assuming 'num_of_delays_total' is the column representing the total number of delays\nfig3 = px.box(df_clean, x='month', y='num_of_delays_total', title=\"Flight Delays By Month\",\n              labels={'num_of_delays_total': 'Delayed Flights', 'month': 'Months'},\n              category_orders={'month': ['January', 'February', 'March', 'April', 'May', 'June',\n                                          'July', 'August', 'September', 'October', 'November', 'December']})\n\n# Customizing layout\nfig3.update_layout(xaxis_title='Months', yaxis_title='Delayed Flights', showlegend=False)\n\n# Show the plot\nfig3.show()\n\nimport plotly.graph_objects as go\n\n# Group by 'month' and calculate the mean of delayed flights\nmonthly_delay_stats = df_clean.groupby('month')['num_of_delays_total'].mean().reset_index()\n\n# Create a table\ntable_fig = go.Figure(data=[go.Table(\n    header=dict(values=['Month', 'Average Delayed Flights']),\n    cells=dict(values=[monthly_delay_stats['month'], monthly_delay_stats['num_of_delays_total']],\n               format=[None, ',.2f']))\n])\n\n# Customize layout\ntable_fig.update_layout(\n    title='Average Delayed Flights by Month',\n    showlegend=False\n)\n\n# Show the table\ntable_fig.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-4",
    "href": "Projects/project2.html#questiontask-4",
    "title": "Client Report - Late flights and Missing Data",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nAccording to the BTS website, the “Weather” category only accounts for severe weather delays. Mild weather delays are not counted in the “Weather” category, but are actually included in both the “NAS” and “Late-Arriving Aircraft” categories. Your job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild). Use these three rules for your calculations:\nA. 100% of delayed flights in the Weather category are due to weather\nB. 30% of all delayed flights in the Late-Arriving category are due to weather.\nC. From April to August, 40% of delayed flights in the NAS category are due to weather. The rest of the months, the proportion rises to 65%.\n\n\nShow the code\n# Assuming df_clean is your original DataFrame\ndf_clean['severe'] = df_clean['num_of_delays_weather']  # no missing values for severe weather delays\n\n# Handling missing values for Late-Arriving category\ndf_clean['mild_late'] = df_clean['num_of_delays_late_aircraft'].replace(-999, np.nan).fillna(df_clean['num_of_delays_late_aircraft'].mean()) * 0.3\n\n# Calculating mild weather delays for NAS category based on month\ndf_clean['mild'] = np.where(\n    df_clean['month'].isin(['April', 'May', 'June', 'July', 'August']),\n    df_clean['num_of_delays_nas'] * 0.4,\n    df_clean['num_of_delays_nas'] * 0.65\n)\n\n# Calculating total weather delays\ndf_clean['weather'] = df_clean['num_of_delays_weather'] + df_clean['mild_late'] + df_clean['mild']\n\n# Calculating proportion of weather delays to total delays and total flights\ndf_clean['proportion_weather_delay'] = df_clean['weather'] / df_clean['num_of_delays_total']\ndf_clean['proportion_weather_total'] = df_clean['weather'] / df_clean['num_of_flights_total']\n\n# Selecting relevant columns\nweather = df_clean[['airport_code', 'month', 'year', 'severe', 'mild', 'mild_late', 'weather', 'proportion_weather_total', 'proportion_weather_delay', 'num_of_flights_total', 'num_of_delays_total']]\nweather.head()\n\n\n\n\n\n\n\n\n\nairport_code\nmonth\nyear\nsevere\nmild\nmild_late\nweather\nproportion_weather_total\nproportion_weather_delay\nnum_of_flights_total\nnum_of_delays_total\n\n\n\n\n0\nATL\nJanuary\n2005.0\n448\n2988.70\n0.0\n3436.70\n0.098057\n0.411335\n35048\n8355\n\n\n1\nDEN\nJanuary\n2005.0\n233\n607.75\n278.4\n1119.15\n0.088212\n0.354948\n12687\n3153\n\n\n2\nIAD\nJanuary\n2005.0\n61\n581.75\n317.4\n960.15\n0.077550\n0.395123\n12381\n2430\n\n\n3\nORD\nJanuary\n2005.0\n306\n3519.75\n676.5\n4502.25\n0.159688\n0.490548\n28194\n9178\n\n\n4\nSAN\nJanuary\n2005.0\n56\n414.70\n204.0\n674.70\n0.092640\n0.345645\n7283\n1952",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-5",
    "href": "Projects/project2.html#questiontask-5",
    "title": "Client Report - Late flights and Missing Data",
    "section": "QUESTION|TASK 5",
    "text": "QUESTION|TASK 5\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Discuss what you learn from this graph.\nLooking at this chart it is insightful as to how impactful weather is on flights. Despite our massive leaps in technology we still must wait on nature. An average on about 1/3 of our delayed flights are due to adverse weather conditions\n\n\nShow the code\nprop_mean = weather.groupby('airport_code')['proportion_weather_delay'].mean()\n\nimport plotly.express as px\n\n# Assuming prop_mean is calculated\nfig4 = px.bar(prop_mean.reset_index(), x='airport_code', y='proportion_weather_delay',\n              title=\"Mean Proportion of Delays Due To Weather For Each Airport\",\n              labels={'proportion_weather_delay': 'Mean Proportion of Weather Delays'},\n              color='proportion_weather_delay',  # Color by the proportion for a gradient effect\n              color_continuous_scale='Viridis',  # You can choose other color scales\n              height=500,  # Adjust the height of the chart\n              width=800,   # Adjust the width of the chart\n              text='proportion_weather_delay',  # Display the values on top of the bars\n)\n\nfig4.update_xaxes(title=\"Airport\", showgrid=False)  # Remove x-axis grid lines for a cleaner look\nfig4.update_yaxes(title=\"Proportion Of Weather Delays\", showgrid=False)  # Remove y-axis grid lines\n\n# Add a color bar for reference\nfig4.update_layout(coloraxis_colorbar=dict(title='Proportion', tickvals=[0, 0.5, 1]))\n\n# Adjust the bar text position\nfig4.update_traces(textposition='outside', insidetextanchor='start')\n\nfig4.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 1\nProject 2\nProject 3\nProject 4\nProject 5",
    "crumbs": [
      "DS250 Projects"
    ]
  },
  {
    "objectID": "projects.html#repo-for-all-my-projects",
    "href": "projects.html#repo-for-all-my-projects",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 1\nProject 2\nProject 3\nProject 4\nProject 5",
    "crumbs": [
      "DS250 Projects"
    ]
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html#title-2-header",
    "href": "full_stack.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  }
]